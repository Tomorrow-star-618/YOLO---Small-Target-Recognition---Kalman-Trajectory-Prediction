#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
åŸºäºå±€éƒ¨ç°åº¦å€¼çš„ç›®æ ‡è¿½è¸ªç³»ç»Ÿ
ç»“åˆYOLOæ£€æµ‹å’Œç°åº¦åŒºåŸŸé¢„æµ‹è¿›è¡Œç›®æ ‡è¿½è¸ª

ä½¿ç”¨æ–¹æ³• / Usage:
-----------------
python grayscale_tracking_system.py --video <è§†é¢‘è·¯å¾„> [å…¶ä»–å‚æ•°]

å¿…éœ€å‚æ•° / Required Arguments:
  --video, -v <PATH>         è¾“å…¥è§†é¢‘æ–‡ä»¶è·¯å¾„ (å¿…å¡«)

å¯é€‰å‚æ•° / Optional Arguments:
  --model, -m <PATH>         YOLOæ¨¡å‹æ–‡ä»¶è·¯å¾„
                            é»˜è®¤: small_target_detection/yolov8_small_aircraft/weights/best.pt
  --output, -o <PATH>        è¾“å‡ºè§†é¢‘æ–‡ä»¶è·¯å¾„ (å¯é€‰ï¼Œé»˜è®¤è‡ªåŠ¨ç”Ÿæˆ)
  --template, -t <STR>       å±€éƒ¨ç°åº¦å€¼æ¨¡æ¿ (å¯é€‰ï¼Œæ ¼å¼ä¸ºæ•°ç»„å­—ç¬¦ä¸²)
  --test <START,END>         æµ‹è¯•æ¨¡å¼ï¼ŒæŒ‡å®šå¼ºåˆ¶ä¸¢å¤±å¸§èŒƒå›´ (ä¾‹å¦‚: 50,100)
  --save-process             ä¿å­˜å¤„ç†è¿‡ç¨‹ä¸­çš„ROIå›¾åƒå’Œç°åº¦çŸ©é˜µæ•°æ®

ä½¿ç”¨ç¤ºä¾‹ / Examples:
------------------
# åŸºæœ¬ä½¿ç”¨ - ä½¿ç”¨é»˜è®¤æ¨¡å‹å¤„ç†è§†é¢‘
python grayscale_tracking_system.py --video vedio/test.mp4

# æŒ‡å®šæ¨¡å‹å’Œä¿å­˜å¤„ç†è¿‡ç¨‹
python grayscale_tracking_system.py --video vedio/test.mp4 --model yolo11x.pt --save-process

# æµ‹è¯•æ¨¡å¼ - åœ¨æŒ‡å®šå¸§èŒƒå›´å¼ºåˆ¶ç›®æ ‡ä¸¢å¤±
python grayscale_tracking_system.py --video vedio/test.mp4 --test 30,80

# å®Œæ•´å‚æ•°ç¤ºä¾‹
python grayscale_tracking_system.py \
    --video vedio/aircraft.mp4 \
    --model yolo11x.pt \
    --output results/tracked_aircraft.mp4 \
    --test 50,150 \
    --save-process

è¾“å‡ºè¯´æ˜ / Output:
----------------
ç¨‹åºå°†åœ¨ Grayscale-Tracking/runs/ ç›®å½•ä¸‹è‡ªåŠ¨åˆ›å»ºä»¥ä¸‹ç»“æ„:
  è§†é¢‘å_æ—¥æœŸæ—¶é—´/
  â”œâ”€â”€ output-video/          # è¾“å‡ºçš„è¿½è¸ªè§†é¢‘
  â””â”€â”€ process/              # å¤„ç†è¿‡ç¨‹æ–‡ä»¶ (å¦‚æœå¯ç”¨ --save-process)
      â”œâ”€â”€ roi_patches/      # ROIå›¾åƒå—
      â””â”€â”€ grayscale_data/   # ç°åº¦çŸ©é˜µæ•°æ®å’Œå¯¹æ¯”å›¾

åŠŸèƒ½ç‰¹æ€§ / Features:
------------------
âœ“ YOLOç›®æ ‡æ£€æµ‹ä¸ç°åº¦é¢„æµ‹ç»“åˆ
âœ“ GPUåŠ é€Ÿå¤„ç† (è‡ªåŠ¨æ£€æµ‹CUDA)  
âœ“ ä¸¢å¤±åæŒç»­é¢„æµ‹ç›´åˆ°é‡æ–°æ£€æµ‹
âœ“ å®æ—¶è¿›åº¦æ¡å’ŒFPSæ˜¾ç¤º
âœ“ è‡ªåŠ¨ç›®å½•ç»“æ„ç®¡ç†
âœ“ å¯è§†åŒ–å¤„ç†è¿‡ç¨‹ä¿å­˜
âœ“ æµ‹è¯•æ¨¡å¼æ”¯æŒ
"""

import cv2
import numpy as np
import torch
from ultralytics import YOLO
import os
from pathlib import Path
import time
from collections import defaultdict
import argparse

class GrayscaleTracker:
    """åŸºäºç°åº¦å€¼çš„ç›®æ ‡è¿½è¸ªå™¨"""
    
    def __init__(self, model_path, local_grayscale_template=None, save_process=False):
        """åˆå§‹åŒ–è¿½è¸ªå™¨
        
        Args:
            model_path: YOLOæ¨¡å‹è·¯å¾„
            local_grayscale_template: å±€éƒ¨ç°åº¦å€¼æ¨¡æ¿ (25x25 numpyæ•°ç»„)
            save_process: æ˜¯å¦ä¿å­˜å¤„ç†è¿‡ç¨‹å›¾åƒ
        """
        self.model = YOLO(model_path)
        self.local_grayscale_template = local_grayscale_template
        self.save_process = save_process
        
        # è¿½è¸ªå‚æ•°
        self.roi_size = 40  # ROIåŒºåŸŸå¤§å°
        self.search_radius = 50  # æœç´¢åŠå¾„
        self.min_prediction_confidence = 0.1  # æœ€ä½é¢„æµ‹ç½®ä¿¡åº¦é˜ˆå€¼
        
        # GPUåŠ é€Ÿè®¾ç½®
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.use_gpu = torch.cuda.is_available()
        if self.use_gpu:
            print(f"ğŸš€ GPUåŠ é€Ÿå·²å¯ç”¨: {torch.cuda.get_device_name()}")
        else:
            print("âš ï¸ GPUä¸å¯ç”¨ï¼Œä½¿ç”¨CPUå¤„ç†")
        
        # è¿½è¸ªçŠ¶æ€
        self.tracks = defaultdict(dict)  # è½¨è¿¹ä¿¡æ¯
        self.track_id_counter = 0
        self.video_fps = 30  # è§†é¢‘å¸§ç‡ï¼Œå¤„ç†è§†é¢‘æ—¶ä¼šæ›´æ–°
        
        # åˆ›å»ºå¤„ç†è¿‡ç¨‹ä¿å­˜ç›®å½•
        if self.save_process:
            # æš‚æ—¶è®¾ç½®é»˜è®¤ç›®å½•ï¼Œå®é™…ç›®å½•åœ¨process_videoä¸­åˆ›å»º
            self.process_dir = None
            self.roi_patches_dir = None
            self.grayscale_data_dir = None
        
        print(f"âœ… åˆå§‹åŒ–ç°åº¦è¿½è¸ªç³»ç»Ÿ")
        print(f"   æ¨¡å‹è·¯å¾„: {model_path}")
        print(f"   ROIå¤§å°: {self.roi_size}x{self.roi_size}")
        print(f"   æœ€ä½é¢„æµ‹ç½®ä¿¡åº¦: {self.min_prediction_confidence}")
        print(f"   ä¿å­˜è¿‡ç¨‹: {'æ˜¯' if save_process else 'å¦'}")
        print(f"   è¿½è¸ªç­–ç•¥: æŒç»­é¢„æµ‹ç›´åˆ°é‡æ–°æ£€æµ‹åˆ°ç›®æ ‡")
        print(f"   GPUåŠ é€Ÿ: {'å¯ç”¨' if self.use_gpu else 'ç¦ç”¨'} ({'CUDA' if self.use_gpu else 'CPU'})")
    
    def create_results_directory(self, video_path):
        """åˆ›å»ºç»“æœç›®å½•ï¼ŒåŸºäºè§†é¢‘åç§°å’Œå½“å‰æ—¥æœŸæ—¶é—´
        
        Args:
            video_path: è§†é¢‘æ–‡ä»¶è·¯å¾„
            
        Returns:
            results_dir: åˆ›å»ºçš„ç»“æœç›®å½•è·¯å¾„
        """
        import datetime
        
        # è·å–è§†é¢‘æ–‡ä»¶åï¼ˆä¸å«æ‰©å±•åï¼‰
        video_name = Path(video_path).stem
        
        # è·å–å½“å‰æ—¥æœŸæ—¶é—´
        now = datetime.datetime.now()
        timestamp = now.strftime("%Y%m%d_%H%M%S")
        
        # åˆ›å»ºç»“æœç›®å½•å
        results_dir_name = f"{video_name}_{timestamp}"
        
        # åœ¨Grayscale-Tracking/runsç›®å½•ä¸‹åˆ›å»ºç»“æœç›®å½•
        script_dir = Path(__file__).parent
        runs_dir = script_dir / "runs"
        runs_dir.mkdir(exist_ok=True)  # ç¡®ä¿runsç›®å½•å­˜åœ¨
        results_dir = runs_dir / results_dir_name
        
        # åˆ›å»ºä¸»ç›®å½•å’Œå­ç›®å½•
        results_dir.mkdir(exist_ok=True)
        
        output_video_dir = results_dir / "output-video"
        process_dir = results_dir / "process"
        
        output_video_dir.mkdir(exist_ok=True)
        process_dir.mkdir(exist_ok=True)
        
        # å¦‚æœéœ€è¦ä¿å­˜å¤„ç†è¿‡ç¨‹ï¼Œåˆ›å»ºå­ç›®å½•
        if self.save_process:
            self.process_dir = process_dir
            self.roi_patches_dir = process_dir / "roi_patches"
            self.grayscale_data_dir = process_dir / "grayscale_data"
            self.roi_patches_dir.mkdir(exist_ok=True)
            self.grayscale_data_dir.mkdir(exist_ok=True)
        
        print(f"ğŸ“ åˆ›å»ºç»“æœç›®å½•: {results_dir}")
        print(f"   - è¾“å‡ºè§†é¢‘: {output_video_dir}")
        if self.save_process:
            print(f"   - å¤„ç†è¿‡ç¨‹: {process_dir}")
        
        return results_dir, output_video_dir, process_dir

    def save_process_images(self, frame, track_id, frame_id, roi_center, roi_data, prediction_type="gradient", 
                           last_detection_info=None):
        """ä¿å­˜å¤„ç†è¿‡ç¨‹ä¸­çš„å›¾åƒå’Œæ•°æ®ï¼ŒåŒ…å«ä¸¢å¤±å‰åçš„å¯¹æ¯”
        
        Args:
            frame: åŸå§‹å¸§
            track_id: è½¨è¿¹ID
            frame_id: å¸§ID
            roi_center: ROIä¸­å¿ƒä½ç½® (x, y) åœ¨æ•´ä¸ªå›¾åƒä¸­çš„ä½ç½®
            roi_data: ROIåŒºåŸŸæ•°æ®
            prediction_type: é¢„æµ‹ç±»å‹ ("gradient" æˆ– "template")
            last_detection_info: æœ€åä¸€æ¬¡æ£€æµ‹çš„ä¿¡æ¯ {"center": (x,y), "roi": np.array, "frame_id": int, "confidence": float}
        """
        if not self.save_process:
            return
        
        import matplotlib
        matplotlib.use('Agg')  # ä½¿ç”¨éäº¤äº’å¼åç«¯ï¼Œé¿å…Qtä¾èµ–é—®é¢˜
        import matplotlib.pyplot as plt
        
        # è®¡ç®—è§†é¢‘ç§’æ•°
        video_seconds = frame_id / self.video_fps
        
        # ROIå°æ–¹å—å†…çš„ä¸­å¿ƒä½ç½® (ç›¸å¯¹äºROIåŒºåŸŸçš„ä¸­å¿ƒ)
        roi_local_center = (roi_data.shape[1] // 2, roi_data.shape[0] // 2)
        
        # 1. ä¿å­˜ROIæ–¹å—å›¾åƒ - å‘½åï¼šç§’æ•°+å¸§æ•°+ä¸­å¿ƒä½ç½®
        roi_filename = f"{video_seconds:.1f}s_f{frame_id:04d}_center{roi_center[0]}-{roi_center[1]}_roi.png"
        roi_path = self.roi_patches_dir / roi_filename
        
        # ä¿å­˜åŸå§‹ROIå›¾åƒ
        cv2.imwrite(str(roi_path), roi_data)
        
        # 2. ä¿å­˜ç°åº¦çŸ©é˜µæ•°æ®å›¾åƒ - æ”¯æŒå¯¹æ¯”æ˜¾ç¤º
        data_filename = f"{video_seconds:.1f}s_f{frame_id:04d}_gray_center{roi_center[0]}-{roi_center[1]}_comparison.png"
        data_path = self.grayscale_data_dir / data_filename
        
        # åˆ›å»ºå¯¹æ¯”å›¾åƒ
        if last_detection_info is not None:
            # æœ‰æœ€åæ£€æµ‹ä¿¡æ¯ï¼Œåˆ›å»ºå¯¹æ¯”å›¾
            fig, axes = plt.subplots(2, 3, figsize=(15, 10))
            
            # ä¸Šæ’ï¼šæœ€åæ£€æµ‹çš„ROI
            last_roi = last_detection_info["roi"]
            last_center = last_detection_info["center"]
            last_frame_id = last_detection_info["frame_id"]
            last_confidence = last_detection_info["confidence"]
            last_seconds = last_frame_id / self.video_fps
            
            axes[0, 0].imshow(last_roi, cmap='gray')
            axes[0, 0].set_title(f'Last Detection ROI\nFrame {last_frame_id} ({last_seconds:.1f}s)\nConf: {last_confidence:.3f}')
            axes[0, 0].plot(last_roi.shape[1]//2, last_roi.shape[0]//2, 'r+', markersize=10, markeredgewidth=2)
            axes[0, 0].text(last_roi.shape[1]//2, last_roi.shape[0]//2 + 3, f'({last_center[0]},{last_center[1]})', 
                          ha='center', va='top', color='red', fontsize=8, weight='bold')
            
            axes[0, 1].imshow(last_roi, cmap='hot', interpolation='nearest')
            axes[0, 1].set_title(f'Last Detection Heatmap')
            im1 = axes[0, 1].imshow(last_roi, cmap='hot')
            plt.colorbar(im1, ax=axes[0, 1])
            
            axes[0, 2].contour(last_roi, levels=10)
            axes[0, 2].set_title(f'Last Detection Contours')
            
            # ä¸‹æ’ï¼šå½“å‰é¢„æµ‹çš„ROI
            axes[1, 0].imshow(roi_data, cmap='gray')
            axes[1, 0].set_title(f'Current Prediction ROI\nFrame {frame_id} ({video_seconds:.1f}s)\n{prediction_type.title()}')
            axes[1, 0].plot(roi_data.shape[1]//2, roi_data.shape[0]//2, 'r+', markersize=10, markeredgewidth=2)
            axes[1, 0].text(roi_data.shape[1]//2, roi_data.shape[0]//2 + 3, f'({roi_center[0]},{roi_center[1]})', 
                          ha='center', va='top', color='red', fontsize=8, weight='bold')
            
            axes[1, 1].imshow(roi_data, cmap='hot', interpolation='nearest')
            axes[1, 1].set_title(f'Current Prediction Heatmap')
            im2 = axes[1, 1].imshow(roi_data, cmap='hot')
            plt.colorbar(im2, ax=axes[1, 1])
            
            axes[1, 2].contour(roi_data, levels=10)
            axes[1, 2].set_title(f'Current Prediction Contours')
            
            plt.suptitle(f'Track {track_id} - Detection vs Prediction Comparison\n'
                        f'Lost Frames: {frame_id - last_frame_id}', fontsize=14)
        else:
            # æ²¡æœ‰æœ€åæ£€æµ‹ä¿¡æ¯ï¼Œä½¿ç”¨åŸæ¥çš„å¸ƒå±€
            fig, axes = plt.subplots(2, 2, figsize=(10, 8))
            
            axes[0, 0].imshow(roi_data, cmap='gray')
            axes[0, 0].set_title(f'ROI Image ({roi_data.shape[0]}x{roi_data.shape[1]})')
            axes[0, 0].plot(roi_data.shape[1]//2, roi_data.shape[0]//2, 'r+', markersize=10, markeredgewidth=2)
            axes[0, 0].text(roi_data.shape[1]//2, roi_data.shape[0]//2 + 3, f'({roi_center[0]},{roi_center[1]})', 
                          ha='center', va='top', color='red', fontsize=8, weight='bold')
            
            axes[0, 1].imshow(roi_data, cmap='hot', interpolation='nearest')
            axes[0, 1].set_title('Grayscale Heatmap')
            im = axes[0, 1].imshow(roi_data, cmap='hot')
            plt.colorbar(im, ax=axes[0, 1])
            
            axes[1, 0].contour(roi_data, levels=10)
            axes[1, 0].set_title('Grayscale Contours')
            
            axes[1, 1].axis('off')
            # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
            stats_text = f"""Statistics:
Shape: {roi_data.shape}
Min: {np.min(roi_data)}
Max: {np.max(roi_data)}
Mean: {np.mean(roi_data):.1f}
Std: {np.std(roi_data):.1f}
Center: {roi_center}"""
            axes[1, 1].text(0.1, 0.9, stats_text, 
                           transform=axes[1, 1].transAxes, fontsize=10, verticalalignment='top')
            
            plt.suptitle(f'Track {track_id} - Frame {frame_id} - {prediction_type.title()} Prediction')
        
        plt.tight_layout()
        plt.savefig(data_path, dpi=150, bbox_inches='tight')
        plt.close()
        
        # 3. ä¿å­˜æ•°å€¼æ•°æ®åˆ°txtæ–‡ä»¶ - å‘½åï¼šç§’æ•°+å¸§æ•°+ç°åº¦ä¸­å¿ƒä½ç½®
        txt_filename = f"{video_seconds:.1f}s_f{frame_id:04d}_gray_local{roi_local_center[0]}-{roi_local_center[1]}_global{roi_center[0]}-{roi_center[1]}_matrix.txt"
        txt_path = self.grayscale_data_dir / txt_filename
        
        with open(txt_path, 'w') as f:
            f.write(f"Track ID: {track_id}\n")
            f.write(f"Frame ID: {frame_id}\n")
            f.write(f"Video Time: {video_seconds:.1f}s\n")
            f.write(f"Prediction Type: {prediction_type}\n")
            f.write(f"ROI Center (Global): {roi_center}\n")
            f.write(f"ROI Center (Local): {roi_local_center}\n")
            f.write(f"ROI Shape: {roi_data.shape}\n")
            f.write(f"Min Value: {np.min(roi_data)}\n")
            f.write(f"Max Value: {np.max(roi_data)}\n")
            f.write(f"Mean Value: {np.mean(roi_data):.2f}\n")
            f.write(f"Std Value: {np.std(roi_data):.2f}\n")
            f.write(f"\nGrayscale Matrix:\n")
            
            for i, row in enumerate(roi_data):
                row_str = ' '.join([f'{val:3d}' for val in row])
                f.write(f"Row {i:2d}: [{row_str}]\n")
        
        print(f"ğŸ’¾ ä¿å­˜å¤„ç†è¿‡ç¨‹: {video_seconds:.1f}s Frame{frame_id} -> {roi_filename}, {data_filename}, {txt_filename}")

    def set_template(self, template):
        """è®¾ç½®å±€éƒ¨ç°åº¦å€¼æ¨¡æ¿"""
        if isinstance(template, list):
            template = np.array(template)
        self.local_grayscale_template = template
        print(f"âœ… è®¾ç½®ç°åº¦æ¨¡æ¿: {template.shape}")
    
    def yolo_detect(self, frame, force_loss_frames=None):
        """YOLOç›®æ ‡æ£€æµ‹
        
        Args:
            frame: è¾“å…¥å¸§
            force_loss_frames: å¼ºåˆ¶ç›®æ ‡ä¸¢å¤±çš„å¸§èŒƒå›´ (start_frame, end_frame)ï¼Œç”¨äºæµ‹è¯•
            
        Returns:
            æ£€æµ‹ç»“æœåˆ—è¡¨ [(x1, y1, x2, y2, conf, class_id), ...]
        """
        results = self.model(frame, verbose=False)
        detections = []
        
        for result in results:
            if result.boxes is not None:
                boxes = result.boxes.xyxy.cpu().numpy()  # x1, y1, x2, y2
                confs = result.boxes.conf.cpu().numpy()
                classes = result.boxes.cls.cpu().numpy()
                
                for box, conf, cls in zip(boxes, confs, classes):
                    x1, y1, x2, y2 = map(int, box)
                    detections.append((x1, y1, x2, y2, conf, int(cls)))
        
        # å¦‚æœè®¾ç½®äº†å¼ºåˆ¶ä¸¢å¤±å¸§èŒƒå›´ï¼Œåœ¨è¯¥èŒƒå›´å†…è¿”å›ç©ºæ£€æµ‹ï¼ˆç”¨äºæµ‹è¯•ï¼‰
        if (force_loss_frames is not None and 
            hasattr(self, 'current_frame_id') and
            force_loss_frames[0] <= self.current_frame_id <= force_loss_frames[1]):
            print(f"ğŸ§ª æµ‹è¯•æ¨¡å¼: å¼ºåˆ¶ç›®æ ‡ä¸¢å¤± (å¸§ {self.current_frame_id})")
            return []
        
        return detections
    
    def calculate_center(self, x1, y1, x2, y2):
        """è®¡ç®—è¾¹ç•Œæ¡†ä¸­å¿ƒç‚¹"""
        return int((x1 + x2) / 2), int((y1 + y2) / 2)
    
    def extract_roi(self, frame, center_x, center_y, size=None):
        """æå–ROIåŒºåŸŸ
        
        Args:
            frame: è¾“å…¥å¸§
            center_x, center_y: ä¸­å¿ƒç‚¹åæ ‡
            size: ROIå°ºå¯¸
            
        Returns:
            roi: ROIåŒºåŸŸå›¾åƒ
            roi_coords: ROIåæ ‡ (x1, y1, x2, y2)
        """
        if size is None:
            size = self.roi_size
        
        half_size = size // 2
        h, w = frame.shape[:2]
        
        # è®¡ç®—ROIè¾¹ç•Œï¼Œç¡®ä¿ä¸è¶Šç•Œ
        x1 = max(0, center_x - half_size)
        y1 = max(0, center_y - half_size)
        x2 = min(w, center_x + half_size)
        y2 = min(h, center_y + half_size)
        
        roi = frame[y1:y2, x1:x2]
        return roi, (x1, y1, x2, y2)
    
    def template_matching(self, frame, last_center, search_radius=None):
        """åŸºäºæ¨¡æ¿åŒ¹é…çš„ä½ç½®é¢„æµ‹
        
        Args:
            frame: å½“å‰å¸§
            last_center: ä¸Šä¸€å¸§ä¸­å¿ƒä½ç½®
            search_radius: æœç´¢åŠå¾„
            
        Returns:
            predicted_center: é¢„æµ‹çš„ä¸­å¿ƒä½ç½®
            match_score: åŒ¹é…å¾—åˆ†
        """
        if self.local_grayscale_template is None:
            return last_center, 0.0
        
        if search_radius is None:
            search_radius = self.search_radius
        
        last_x, last_y = last_center
        h, w = frame.shape[:2]
        
        # å°†å½©è‰²å¸§è½¬ä¸ºç°åº¦
        if len(frame.shape) == 3:
            gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        else:
            gray_frame = frame
        
        # å®šä¹‰æœç´¢åŒºåŸŸ
        search_x1 = max(0, last_x - search_radius)
        search_y1 = max(0, last_y - search_radius)
        search_x2 = min(w, last_x + search_radius)
        search_y2 = min(h, last_y + search_radius)
        
        search_region = gray_frame[search_y1:search_y2, search_x1:search_x2]
        
        if search_region.size == 0:
            return last_center, 0.0
        
        # ç¡®ä¿æ¨¡æ¿å°ºå¯¸åˆé€‚
        template = self.local_grayscale_template.astype(np.uint8)
        
        # æ¨¡æ¿åŒ¹é…
        result = cv2.matchTemplate(search_region, template, cv2.TM_CCOEFF_NORMED)
        
        if result.size == 0:
            return last_center, 0.0
        
        # æ‰¾åˆ°æœ€ä½³åŒ¹é…ä½ç½®
        min_val, max_val, min_loc, max_loc = cv2.minMaxLoc(result)
        
        # è½¬æ¢åˆ°åŸå§‹åæ ‡ç³»
        match_x = search_x1 + max_loc[0] + template.shape[1] // 2
        match_y = search_y1 + max_loc[1] + template.shape[0] // 2
        
        return (match_x, match_y), max_val
    
    def gradient_magnitude_prediction(self, frame, last_center, search_radius=None):
        """åŸºäºå±€éƒ¨ç°åº¦å€¼çš„ä½ç½®é¢„æµ‹
        
        é€»è¾‘ï¼š
        1. ä»¥æœ€åæ£€æµ‹ä¸­å¿ƒä¸ºåŸºå‡†ï¼Œæå–40x40çš„ROIåŒºåŸŸ
        2. åœ¨40x40åŒºåŸŸå†…å¯»æ‰¾5x5çª—å£ä¸­ç°åº¦å€¼æœ€é«˜çš„ä½ç½®
        3. è¯¥5x5çª—å£çš„ä¸­å¿ƒå°±æ˜¯é¢„æµ‹çš„æ–°ä¸­å¿ƒä½ç½®
        
        Args:
            frame: å½“å‰å¸§
            last_center: ä¸Šä¸€å¸§ä¸­å¿ƒä½ç½® (æœ€åæ£€æµ‹åˆ°çš„ä½ç½®)
            search_radius: æœç´¢åŠå¾„ (æ­¤æ–¹æ³•ä¸­ä¸ä½¿ç”¨ï¼Œä¿æŒæ¥å£ä¸€è‡´)
            
        Returns:
            best_center: é¢„æµ‹çš„ä¸­å¿ƒä½ç½®
            best_score: æœ€ä½³åŒ¹é…å¾—åˆ†
        """
        last_x, last_y = last_center
        h, w = frame.shape[:2]
        
        # å°†å½©è‰²å¸§è½¬ä¸ºç°åº¦
        if len(frame.shape) == 3:
            gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        else:
            gray_frame = frame
        
        # æ­¥éª¤1: ä»¥æœ€åæ£€æµ‹ä¸­å¿ƒä¸ºåŸºå‡†ï¼Œæå–40x40çš„ROIåŒºåŸŸ
        roi_size = self.roi_size  # 40x40
        half_roi = roi_size // 2  # 20
        
        # è®¡ç®—ROIè¾¹ç•Œï¼Œç¡®ä¿ä¸è¶Šç•Œ
        roi_x1 = max(0, last_x - half_roi)
        roi_y1 = max(0, last_y - half_roi)
        roi_x2 = min(w, last_x + half_roi)
        roi_y2 = min(h, last_y + half_roi)
        
        # æå–40x40çš„ROIåŒºåŸŸ
        roi_40x40 = gray_frame[roi_y1:roi_y2, roi_x1:roi_x2]
        
        if roi_40x40.size == 0:
            print(f"âš ï¸ ROIåŒºåŸŸä¸ºç©ºï¼Œè¿”å›åŸä½ç½® {last_center}")
            return last_center, 0.0
        
        # æ­¥éª¤2: åœ¨40x40åŒºåŸŸå†…å¯»æ‰¾5x5çª—å£ä¸­ç°åº¦å€¼æœ€é«˜çš„ä½ç½®
        window_size = 5
        half_window = window_size // 2  # 2
        
        best_score = -1
        best_local_center = (roi_40x40.shape[1] // 2, roi_40x40.shape[0] // 2)  # é»˜è®¤ä¸­å¿ƒ
        
        # åœ¨40x40åŒºåŸŸå†…æ»‘åŠ¨5x5çª—å£
        for y in range(half_window, roi_40x40.shape[0] - half_window):
            for x in range(half_window, roi_40x40.shape[1] - half_window):
                # æå–5x5çª—å£
                window_5x5 = roi_40x40[y-half_window:y+half_window+1, 
                                     x-half_window:x+half_window+1]
                
                if window_5x5.shape != (window_size, window_size):
                    continue
                
                # è®¡ç®—5x5çª—å£çš„ç°åº¦è¯„åˆ†ï¼ˆå¹³å‡ç°åº¦å€¼ï¼‰
                window_mean = np.mean(window_5x5.astype(np.float32))
                
                # ä¹Ÿå¯ä»¥è€ƒè™‘æ¢¯åº¦ä¿¡æ¯å¢å¼ºè¯„åˆ†
                grad_x = cv2.Sobel(window_5x5.astype(np.float32), cv2.CV_64F, 1, 0, ksize=3)
                grad_y = cv2.Sobel(window_5x5.astype(np.float32), cv2.CV_64F, 0, 1, ksize=3)
                gradient_magnitude = np.sqrt(grad_x**2 + grad_y**2)
                grad_mean = np.mean(gradient_magnitude)
                
                # ç»¼åˆè¯„åˆ†ï¼šç°åº¦å€¼ + æ¢¯åº¦æƒé‡
                score = window_mean + (grad_mean * 0.3)
                
                if score > best_score:
                    best_score = score
                    best_local_center = (x, y)  # åœ¨40x40åŒºåŸŸå†…çš„åæ ‡
        
        # æ­¥éª¤3: å°†å±€éƒ¨åæ ‡è½¬æ¢ä¸ºå…¨å±€åæ ‡
        # best_local_centeræ˜¯åœ¨40x40 ROIå†…çš„åæ ‡ï¼Œéœ€è¦è½¬æ¢ä¸ºå…¨å›¾åæ ‡
        global_x = roi_x1 + best_local_center[0]
        global_y = roi_y1 + best_local_center[1]
        
        predicted_center = (global_x, global_y)
        
        # å½’ä¸€åŒ–è¯„åˆ†åˆ°0-1èŒƒå›´
        normalized_score = min(1.0, best_score / 255.0)
        
        print(f"ğŸ“ ç°åº¦é¢„æµ‹: åŸä¸­å¿ƒ({last_x},{last_y}) -> 40x40 ROI({roi_x1},{roi_y1},{roi_x2},{roi_y2}) -> "
              f"5x5æœ€ä½³ä½ç½®({best_local_center[0]},{best_local_center[1]}) -> å…¨å±€ä½ç½®({global_x},{global_y}), è¯„åˆ†{normalized_score:.3f}")
        
        return predicted_center, normalized_score
    
    def gradient_magnitude_prediction_gpu(self, frame, last_center, search_radius=None):
        """åŸºäºå±€éƒ¨ç°åº¦å€¼çš„ä½ç½®é¢„æµ‹ - GPUåŠ é€Ÿç‰ˆæœ¬
        
        ä½¿ç”¨PyTorchå’ŒGPUå¹¶è¡Œè®¡ç®—æ»‘åŠ¨çª—å£ï¼Œå¤§å¹…æå‡å¤„ç†é€Ÿåº¦
        
        Args:
            frame: å½“å‰å¸§
            last_center: ä¸Šä¸€å¸§ä¸­å¿ƒä½ç½® (æœ€åæ£€æµ‹åˆ°çš„ä½ç½®)
            search_radius: æœç´¢åŠå¾„ (æ­¤æ–¹æ³•ä¸­ä¸ä½¿ç”¨ï¼Œä¿æŒæ¥å£ä¸€è‡´)
            
        Returns:
            best_center: é¢„æµ‹çš„ä¸­å¿ƒä½ç½®
            best_score: æœ€ä½³åŒ¹é…å¾—åˆ†
        """
        last_x, last_y = last_center
        h, w = frame.shape[:2]
        
        # å°†å½©è‰²å¸§è½¬ä¸ºç°åº¦
        if len(frame.shape) == 3:
            gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        else:
            gray_frame = frame
        
        # æ­¥éª¤1: ä»¥æœ€åæ£€æµ‹ä¸­å¿ƒä¸ºåŸºå‡†ï¼Œæå–40x40çš„ROIåŒºåŸŸ
        roi_size = self.roi_size  # 40x40
        half_roi = roi_size // 2  # 20
        
        # è®¡ç®—ROIè¾¹ç•Œï¼Œç¡®ä¿ä¸è¶Šç•Œ
        roi_x1 = max(0, last_x - half_roi)
        roi_y1 = max(0, last_y - half_roi)
        roi_x2 = min(w, last_x + half_roi)
        roi_y2 = min(h, last_y + half_roi)
        
        # æå–40x40çš„ROIåŒºåŸŸ
        roi_40x40 = gray_frame[roi_y1:roi_y2, roi_x1:roi_x2]
        
        if roi_40x40.size == 0:
            print(f"âš ï¸ ROIåŒºåŸŸä¸ºç©ºï¼Œè¿”å›åŸä½ç½® {last_center}")
            return last_center, 0.0
        
        # æ­¥éª¤2: GPUåŠ é€Ÿçš„æ»‘åŠ¨çª—å£æœç´¢
        window_size = 5
        half_window = window_size // 2  # 2
        
        # è½¬æ¢ä¸ºPyTorchå¼ é‡å¹¶ç§»è‡³GPU
        roi_tensor = torch.from_numpy(roi_40x40.astype(np.float32)).unsqueeze(0).unsqueeze(0).to(self.device)
        
        # ä½¿ç”¨unfoldæ“ä½œé«˜æ•ˆæå–æ‰€æœ‰5x5çª—å£
        # unfold(dimension, size, step)
        windows = roi_tensor.unfold(2, window_size, 1).unfold(3, window_size, 1)
        # å½¢çŠ¶: (1, 1, valid_h, valid_w, 5, 5)
        
        if windows.numel() == 0:
            print(f"âš ï¸ æ»‘åŠ¨çª—å£ä¸ºç©ºï¼Œè¿”å›åŸä½ç½® {last_center}")
            return last_center, 0.0
        
        # é‡å¡‘ä¸º (num_windows, 5, 5)
        num_h, num_w = windows.shape[2], windows.shape[3]
        windows = windows.reshape(num_h * num_w, window_size, window_size)
        
        # è®¡ç®—æ¯ä¸ªçª—å£çš„å¹³å‡ç°åº¦å€¼
        window_means = windows.mean(dim=(1, 2))
        
        # è®¡ç®—æ¢¯åº¦ï¼ˆä½¿ç”¨ç®€åŒ–çš„Sobelç®—å­ï¼‰
        # Sobel Xæ ¸
        sobel_x = torch.tensor([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]], dtype=torch.float32, device=self.device)
        sobel_y = torch.tensor([[-1, -2, -1], [0, 0, 0], [1, 2, 1]], dtype=torch.float32, device=self.device)
        
        # å¯¹æ¯ä¸ª5x5çª—å£åº”ç”¨Sobelç®—å­
        grad_x_list = []
        grad_y_list = []
        
        for i in range(windows.shape[0]):
            window = windows[i]
            # åº”ç”¨3x3 Sobelåˆ°5x5çª—å£çš„ä¸­å¿ƒ3x3åŒºåŸŸ
            center_3x3 = window[1:4, 1:4]
            
            # è®¡ç®—æ¢¯åº¦
            grad_x = torch.sum(center_3x3 * sobel_x)
            grad_y = torch.sum(center_3x3 * sobel_y)
            
            grad_x_list.append(grad_x)
            grad_y_list.append(grad_y)
        
        grad_x_tensor = torch.stack(grad_x_list)
        grad_y_tensor = torch.stack(grad_y_list)
        gradient_magnitudes = torch.sqrt(grad_x_tensor**2 + grad_y_tensor**2)
        
        # ç»¼åˆè¯„åˆ†ï¼šç°åº¦å€¼ + æ¢¯åº¦æƒé‡
        scores = window_means + (gradient_magnitudes * 0.3)
        
        # æ‰¾åˆ°æœ€ä½³è¯„åˆ†ä½ç½®
        best_idx = torch.argmax(scores).item()
        best_score = scores[best_idx].item()
        
        # è½¬æ¢ç´¢å¼•å›äºŒç»´åæ ‡ï¼ˆåœ¨æ»‘åŠ¨çª—å£åæ ‡ç³»ä¸­ï¼‰
        best_y = best_idx // num_w
        best_x = best_idx % num_w
        
        # è½¬æ¢ä¸º40x40 ROIå†…çš„åæ ‡ï¼ˆåŠ ä¸Šhalf_windowåç§»ï¼‰
        best_local_center = (best_x + half_window, best_y + half_window)
        
        # æ­¥éª¤3: å°†å±€éƒ¨åæ ‡è½¬æ¢ä¸ºå…¨å±€åæ ‡
        global_x = roi_x1 + best_local_center[0]
        global_y = roi_y1 + best_local_center[1]
        
        predicted_center = (global_x, global_y)
        
        # å½’ä¸€åŒ–è¯„åˆ†åˆ°0-1èŒƒå›´
        normalized_score = min(1.0, best_score / 255.0)
        
        print(f"ğŸš€ GPUåŠ é€Ÿé¢„æµ‹: åŸä¸­å¿ƒ({last_x},{last_y}) -> 40x40 ROI({roi_x1},{roi_y1},{roi_x2},{roi_y2}) -> "
              f"æœ€ä½³ä½ç½®({best_local_center[0]},{best_local_center[1]}) -> å…¨å±€ä½ç½®({global_x},{global_y}), è¯„åˆ†{normalized_score:.3f}")
        
        return predicted_center, normalized_score
    
    def grayscale_similarity_search(self, frame, last_center, search_radius=None):
        """åŸºäºç°åº¦ç›¸ä¼¼æ€§çš„æœç´¢
        
        Args:
            frame: å½“å‰å¸§
            last_center: ä¸Šä¸€å¸§ä¸­å¿ƒä½ç½®
            search_radius: æœç´¢åŠå¾„
            
        Returns:
            best_center: æœ€ä½³åŒ¹é…ä¸­å¿ƒä½ç½®
            best_score: æœ€ä½³åŒ¹é…å¾—åˆ†
        """
        if self.local_grayscale_template is None:
            return last_center, 0.0
        
        if search_radius is None:
            search_radius = self.search_radius
        
        last_x, last_y = last_center
        h, w = frame.shape[:2]
        
        # å°†å½©è‰²å¸§è½¬ä¸ºç°åº¦
        if len(frame.shape) == 3:
            gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        else:
            gray_frame = frame
        
        best_score = -1
        best_center = last_center
        template_size = self.local_grayscale_template.shape[0]
        half_template = template_size // 2
        
        # åœ¨æœç´¢åŠå¾„å†…å¯»æ‰¾æœ€ä½³åŒ¹é…
        for dy in range(-search_radius, search_radius + 1, 2):
            for dx in range(-search_radius, search_radius + 1, 2):
                test_x = last_x + dx
                test_y = last_y + dy
                
                # æ£€æŸ¥è¾¹ç•Œ
                if (test_x - half_template < 0 or test_x + half_template >= w or
                    test_y - half_template < 0 or test_y + half_template >= h):
                    continue
                
                # æå–å€™é€‰ROI
                roi, _ = self.extract_roi(gray_frame, test_x, test_y, template_size)
                
                if roi.shape != self.local_grayscale_template.shape:
                    continue
                
                # è®¡ç®—ç›¸ä¼¼åº¦ï¼ˆå½’ä¸€åŒ–ç›¸å…³ç³»æ•°ï¼‰
                roi_norm = roi.astype(np.float32)
                template_norm = self.local_grayscale_template.astype(np.float32)
                
                # å½’ä¸€åŒ–
                roi_mean = np.mean(roi_norm)
                template_mean = np.mean(template_norm)
                
                roi_centered = roi_norm - roi_mean
                template_centered = template_norm - template_mean
                
                # è®¡ç®—ç›¸å…³ç³»æ•°
                numerator = np.sum(roi_centered * template_centered)
                denominator = np.sqrt(np.sum(roi_centered**2) * np.sum(template_centered**2))
                
                if denominator > 0:
                    score = numerator / denominator
                    if score > best_score:
                        best_score = score
                        best_center = (test_x, test_y)
        
        return best_center, best_score
    
    def associate_detections(self, detections, frame, frame_id):
        """å…³è”æ£€æµ‹ç»“æœåˆ°è½¨è¿¹
        
        Args:
            detections: æ£€æµ‹ç»“æœ
            frame: å½“å‰å¸§å›¾åƒ
            frame_id: å¸§ID
            
        Returns:
            updated_tracks: æ›´æ–°åçš„è½¨è¿¹ä¿¡æ¯
        """
        # ä¸ºæ–°æ£€æµ‹åˆ†é…è½¨è¿¹IDæˆ–æ›´æ–°ç°æœ‰è½¨è¿¹
        current_frame_tracks = {}
        
        for detection in detections:
            x1, y1, x2, y2, conf, cls = detection
            center = self.calculate_center(x1, y1, x2, y2)
            
            # ç®€å•çš„æœ€è¿‘è·ç¦»å…³è”
            best_track_id = None
            min_distance = float('inf')
            
            for track_id, track_info in self.tracks.items():
                if 'last_center' in track_info:
                    last_center = track_info['last_center']
                    distance = np.sqrt((center[0] - last_center[0])**2 + 
                                     (center[1] - last_center[1])**2)
                    if distance < min_distance and distance < 100:  # è·ç¦»é˜ˆå€¼
                        min_distance = distance
                        best_track_id = track_id
            
            if best_track_id is None:
                # åˆ›å»ºæ–°è½¨è¿¹
                best_track_id = self.track_id_counter
                self.track_id_counter += 1
            
            # æ›´æ–°è½¨è¿¹ä¿¡æ¯ï¼Œä¿å­˜æ£€æµ‹æ—¶çš„ROIç”¨äºåç»­å¯¹æ¯”
            roi_x1 = max(0, center[0] - self.roi_size // 2)
            roi_y1 = max(0, center[1] - self.roi_size // 2)
            roi_x2 = min(frame.shape[1], center[0] + self.roi_size // 2)
            roi_y2 = min(frame.shape[0], center[1] + self.roi_size // 2)
            detection_roi = cv2.cvtColor(frame[roi_y1:roi_y2, roi_x1:roi_x2], cv2.COLOR_BGR2GRAY)
            
            self.tracks[best_track_id] = {
                'last_center': center,
                'last_bbox': (x1, y1, x2, y2),
                'last_detection_frame': frame_id,
                'lost_frames': 0,
                'confidence': conf,
                'class_id': cls,
                'status': 'detected',
                'last_detection_roi': detection_roi.copy(),  # ä¿å­˜æ£€æµ‹æ—¶çš„ROI
                'last_detection_info': {
                    'center': center,
                    'roi': detection_roi.copy(),
                    'frame_id': frame_id,
                    'confidence': conf
                }
            }
            
            # è°ƒè¯•ä¿¡æ¯ï¼šYOLOæ£€æµ‹
            if frame_id % 30 == 0:  # æ¯30å¸§è¾“å‡ºä¸€æ¬¡
                print(f"ğŸ¯ YOLOæ£€æµ‹ è½¨è¿¹{best_track_id}: ä¸­å¿ƒ({center[0]}, {center[1]}), "
                      f"è¾¹ç•Œæ¡†({x1}, {y1}, {x2}, {y2}), ç½®ä¿¡åº¦{conf:.3f}")
            
            current_frame_tracks[best_track_id] = self.tracks[best_track_id]
        
        return current_frame_tracks
    
    def predict_lost_targets(self, frame, frame_id):
        """é¢„æµ‹ä¸¢å¤±çš„ç›®æ ‡
        
        Args:
            frame: å½“å‰å¸§
            frame_id: å¸§ID
            
        Returns:
            predicted_tracks: é¢„æµ‹çš„è½¨è¿¹ä¿¡æ¯
        """
        predicted_tracks = {}
        
        for track_id, track_info in list(self.tracks.items()):
            # æ£€æŸ¥æ˜¯å¦ä¸ºä¸¢å¤±çš„è½¨è¿¹
            if track_info['last_detection_frame'] < frame_id:
                lost_frames = frame_id - track_info['last_detection_frame']
                
                # æŒç»­é¢„æµ‹ï¼Œä¸é™åˆ¶æœ€å¤§ä¸¢å¤±å¸§æ•°
                # ä½¿ç”¨è¿ç»­é¢„æµ‹æ–¹æ³•ï¼š
                # - ç¬¬ä¸€æ¬¡é¢„æµ‹ï¼šä½¿ç”¨æœ€åæ£€æµ‹ä½ç½®ä½œä¸ºåŸºå‡†
                # - åç»­é¢„æµ‹ï¼šä½¿ç”¨ä¸Šä¸€æ¬¡é¢„æµ‹ä½ç½®ä½œä¸ºåŸºå‡†ï¼ˆè¿ç»­ä¼ é€’ï¼‰
                current_center = track_info['last_center']
                
                print(f"ğŸ”„ è½¨è¿¹{track_id}æŒç»­é¢„æµ‹: ä¸¢å¤±{lost_frames}å¸§, å½“å‰ä¸­å¿ƒ({current_center[0]}, {current_center[1]})")
                
                # æ–¹æ³•1: æ¢¯åº¦å¹…å€¼é¢„æµ‹ï¼ˆä¸»è¦æ–¹æ³•ï¼‰ - ä¼˜å…ˆä½¿ç”¨GPUåŠ é€Ÿç‰ˆæœ¬
                if self.use_gpu:
                    predicted_center, match_score = self.gradient_magnitude_prediction_gpu(
                        frame, current_center
                    )
                    prediction_type = "gradient_gpu"
                else:
                    predicted_center, match_score = self.gradient_magnitude_prediction(
                        frame, current_center
                    )
                    prediction_type = "gradient_cpu"
                
                # æ–¹æ³•2: å¦‚æœæœ‰ç°åº¦æ¨¡æ¿ï¼Œä½¿ç”¨ç°åº¦ç›¸ä¼¼æ€§ä½œä¸ºè¾…åŠ©
                if self.local_grayscale_template is not None:
                    template_center, template_score = self.grayscale_similarity_search(
                        frame, current_center
                    )
                    # å¦‚æœæ¨¡æ¿åŒ¹é…æ›´å¥½ï¼Œä½¿ç”¨æ¨¡æ¿ç»“æœ
                    if template_score > match_score:
                        predicted_center = template_center
                        match_score = template_score
                        prediction_type = "template"
                
                # ä½¿ç”¨æœ€ä½ç½®ä¿¡åº¦é˜ˆå€¼åˆ¤æ–­æ˜¯å¦ç»§ç»­é¢„æµ‹
                if match_score > self.min_prediction_confidence:
                    # åŸºäºé¢„æµ‹ä¸­å¿ƒç”Ÿæˆè¾¹ç•Œæ¡† - ä½¿ç”¨ä¸Šæ¬¡æ£€æµ‹çš„è¾¹ç•Œæ¡†å¤§å°
                    if 'last_bbox' in track_info:
                        last_x1, last_y1, last_x2, last_y2 = track_info['last_bbox']
                        last_w = last_x2 - last_x1
                        last_h = last_y2 - last_y1
                        pred_x1 = predicted_center[0] - last_w // 2
                        pred_y1 = predicted_center[1] - last_h // 2
                        pred_x2 = predicted_center[0] + last_w // 2
                        pred_y2 = predicted_center[1] + last_h // 2
                    else:
                        # å¦‚æœæ²¡æœ‰å†å²è¾¹ç•Œæ¡†ï¼Œä½¿ç”¨ROIå°ºå¯¸
                        half_size = self.roi_size // 2
                        pred_x1 = predicted_center[0] - half_size
                        pred_y1 = predicted_center[1] - half_size
                        pred_x2 = predicted_center[0] + half_size
                        pred_y2 = predicted_center[1] + half_size
                    
                    # æå–ROIæ•°æ®ç”¨äºä¿å­˜
                    if self.save_process:
                        # è½¬ä¸ºç°åº¦å›¾åƒ
                        if len(frame.shape) == 3:
                            gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
                        else:
                            gray_frame = frame
                        
                        # æå–é¢„æµ‹ä½ç½®çš„ROI
                        roi_data, _ = self.extract_roi(gray_frame, predicted_center[0], predicted_center[1])
                        
                        # ä¿å­˜å¤„ç†è¿‡ç¨‹å›¾åƒ
                        # ä¿å­˜å¤„ç†è¿‡ç¨‹ï¼ŒåŒ…å«æœ€åæ£€æµ‹ä¿¡æ¯ç”¨äºå¯¹æ¯”
                        last_detection_info = track_info.get('last_detection_info', None)
                        self.save_process_images(frame, track_id, frame_id, predicted_center, 
                                               roi_data, prediction_type, last_detection_info)
                    
                    # æ›´æ–°è½¨è¿¹ä¿¡æ¯
                    self.tracks[track_id].update({
                        'last_center': predicted_center,
                        'last_bbox': (pred_x1, pred_y1, pred_x2, pred_y2),
                        'lost_frames': lost_frames,
                        'confidence': match_score,
                        'status': 'predicted'
                    })
                    
                    predicted_tracks[track_id] = self.tracks[track_id]
                    
                    pred_w = pred_x2 - pred_x1
                    pred_h = pred_y2 - pred_y1
                    print(f"ğŸ” è½¨è¿¹{track_id}é¢„æµ‹æˆåŠŸ: ä¸­å¿ƒ({predicted_center[0]}, {predicted_center[1]}), "
                          f"è¾¹ç•Œæ¡†({pred_x1}, {pred_y1}, {pred_x2}, {pred_y2}), å°ºå¯¸{pred_w}x{pred_h}, "
                          f"å¾—åˆ†{match_score:.3f}, ä¸¢å¤±{lost_frames}å¸§, æ–¹æ³•{prediction_type}")
                else:
                    # é¢„æµ‹ç½®ä¿¡åº¦å¤ªä½ï¼Œä½†ä¿æŒè½¨è¿¹ç»§ç»­å°è¯•ä¸‹ä¸€å¸§
                    self.tracks[track_id]['lost_frames'] = lost_frames
                    self.tracks[track_id]['status'] = 'lost_low_confidence'
                    print(f"âš ï¸ è½¨è¿¹{track_id}é¢„æµ‹ç½®ä¿¡åº¦ä½: å¾—åˆ†{match_score:.3f} < {self.min_prediction_confidence}ï¼Œä¿æŒè½¨è¿¹ç»§ç»­å°è¯•")
        
        return predicted_tracks
    
    def draw_tracks(self, frame, detected_tracks, predicted_tracks):
        """ç»˜åˆ¶è½¨è¿¹
        
        Args:
            frame: è¾“å…¥å¸§
            detected_tracks: æ£€æµ‹åˆ°çš„è½¨è¿¹
            predicted_tracks: é¢„æµ‹çš„è½¨è¿¹
            
        Returns:
            annotated_frame: æ ‡æ³¨åçš„å¸§
        """
        annotated_frame = frame.copy()
        
        # ç»˜åˆ¶æ£€æµ‹åˆ°çš„ç›®æ ‡ (ç»¿è‰²)
        for track_id, track_info in detected_tracks.items():
            x1, y1, x2, y2 = track_info['last_bbox']
            conf = track_info['confidence']
            center = track_info['last_center']
            
            # ç»˜åˆ¶è¾¹ç•Œæ¡†
            cv2.rectangle(annotated_frame, (x1, y1), (x2, y2), (0, 255, 0), 2)
            
            # åœ¨è¾¹ç•Œæ¡†ä¸Šæ–¹æ˜¾ç¤ºIDå’Œç½®ä¿¡åº¦
            cv2.putText(annotated_frame, f'ID:{track_id} YOLO:{conf:.2f}', 
                       (x1, y1-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)
            
            # åœ¨ä¸­å¿ƒç‚¹è·Ÿéšæ˜¾ç¤ºç½®ä¿¡åº¦å’Œåæ ‡ (ç»¿è‰²èƒŒæ™¯)
            conf_coord_text = f'{conf:.3f} ({center[0]},{center[1]})'
            text_size = cv2.getTextSize(conf_coord_text, cv2.FONT_HERSHEY_SIMPLEX, 0.4, 1)[0]
            # ç»˜åˆ¶åŠé€æ˜èƒŒæ™¯
            cv2.rectangle(annotated_frame, 
                         (center[0] - text_size[0]//2 - 2, center[1] - text_size[1] - 8),
                         (center[0] + text_size[0]//2 + 2, center[1] - 5),
                         (0, 255, 0), -1)
            # ç»˜åˆ¶ç½®ä¿¡åº¦å’Œåæ ‡æ–‡æœ¬
            cv2.putText(annotated_frame, conf_coord_text,
                       (center[0] - text_size[0]//2, center[1] - 7),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.4, (0, 0, 0), 1)
        
        # ç»˜åˆ¶é¢„æµ‹çš„ç›®æ ‡ (çº¢è‰²)
        for track_id, track_info in predicted_tracks.items():
            x1, y1, x2, y2 = track_info['last_bbox']
            score = track_info['confidence']
            lost_frames = track_info['lost_frames']
            center = track_info['last_center']
            
            # ç»˜åˆ¶è¾¹ç•Œæ¡† - ä½¿ç”¨æ›´ç»†çš„çº¿æ¡(1åƒç´ )ä»¥å‡å°‘é®æŒ¡
            cv2.rectangle(annotated_frame, (x1, y1), (x2, y2), (0, 0, 255), 1)
            
            # åœ¨è¾¹ç•Œæ¡†ä¸Šæ–¹æ˜¾ç¤ºIDã€é¢„æµ‹å¾—åˆ†å’Œä¸¢å¤±å¸§æ•° - ä½¿ç”¨æ›´ç»†çš„æ–‡æœ¬çº¿æ¡
            cv2.putText(annotated_frame, f'ID:{track_id} Pred:{score:.2f} Lost:{lost_frames}', 
                       (x1, y1-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1)
            
            # ç»˜åˆ¶é¢„æµ‹ä¸­å¿ƒç‚¹ - ä½¿ç”¨æ›´å°çš„åœ†ç‚¹
            cv2.circle(annotated_frame, center, 2, (0, 0, 255), -1)
            
            # åœ¨ä¸­å¿ƒç‚¹è·Ÿéšæ˜¾ç¤ºç½®ä¿¡åº¦å’Œåæ ‡ (çº¢è‰²èƒŒæ™¯)
            conf_coord_text = f'{score:.3f} ({center[0]},{center[1]})'
            text_size = cv2.getTextSize(conf_coord_text, cv2.FONT_HERSHEY_SIMPLEX, 0.4, 1)[0]
            # ç»˜åˆ¶åŠé€æ˜èƒŒæ™¯
            cv2.rectangle(annotated_frame, 
                         (center[0] - text_size[0]//2 - 2, center[1] + 5),
                         (center[0] + text_size[0]//2 + 2, center[1] + text_size[1] + 8),
                         (0, 0, 255), -1)
            # ç»˜åˆ¶ç½®ä¿¡åº¦å’Œåæ ‡æ–‡æœ¬
            cv2.putText(annotated_frame, conf_coord_text,
                       (center[0] - text_size[0]//2, center[1] + text_size[1] + 6),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255, 255, 255), 1)
        
        return annotated_frame
    
    def process_video(self, video_path, output_path=None, test_mode=False):
        """å¤„ç†è§†é¢‘
        
        Args:
            video_path: è¾“å…¥è§†é¢‘è·¯å¾„
            output_path: è¾“å‡ºè§†é¢‘è·¯å¾„ï¼ˆå¦‚æœä¸ºNoneï¼Œè‡ªåŠ¨åˆ›å»ºç›®å½•ç»“æ„ï¼‰
            test_mode: æ˜¯å¦å¯ç”¨æµ‹è¯•æ¨¡å¼ï¼ˆå¼ºåˆ¶ç›®æ ‡ä¸¢å¤±ä»¥æµ‹è¯•é¢„æµ‹åŠŸèƒ½ï¼‰
        """
        # åˆ›å»ºç»“æœç›®å½•ç»“æ„
        results_dir, output_video_dir, process_dir = self.create_results_directory(video_path)
        
        # å¦‚æœæœªæŒ‡å®šè¾“å‡ºè·¯å¾„ï¼Œè‡ªåŠ¨ç”Ÿæˆ
        if output_path is None:
            video_name = Path(video_path).stem
            suffix = "_test" if test_mode else ""
            output_path = output_video_dir / f"{video_name}_tracked{suffix}.mp4"
        else:
            # å¦‚æœæŒ‡å®šäº†è¾“å‡ºè·¯å¾„ï¼Œç¡®ä¿å…¶åœ¨æ­£ç¡®çš„ç›®å½•ä¸­
            output_path = output_video_dir / Path(output_path).name
        
        cap = cv2.VideoCapture(video_path)
        if not cap.isOpened():
            raise ValueError(f"æ— æ³•æ‰“å¼€è§†é¢‘: {video_path}")
        
        # è·å–è§†é¢‘å±æ€§
        fps = int(cap.get(cv2.CAP_PROP_FPS))
        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        
        # è®¾ç½®è§†é¢‘å¸§ç‡ç”¨äºæ–‡ä»¶å‘½å
        self.video_fps = fps if fps > 0 else 30  # é»˜è®¤30fps
        
        print(f"ğŸ“¹ å¤„ç†è§†é¢‘: {Path(video_path).name}")
        print(f"   åˆ†è¾¨ç‡: {width}x{height}")
        print(f"   å¸§ç‡: {fps} FPS")
        print(f"   æ€»å¸§æ•°: {total_frames}")
        print(f"   è¾“å‡ºè·¯å¾„: {output_path}")
        # è§£ææµ‹è¯•æ¨¡å¼å‚æ•°
        force_loss_frames = None
        if test_mode:
            try:
                start_frame, end_frame = map(int, test_mode.split(','))
                force_loss_frames = (start_frame, end_frame)
                print(f"ğŸ§ª æµ‹è¯•æ¨¡å¼: å°†åœ¨å¸§{start_frame}-{end_frame}å¼ºåˆ¶ç›®æ ‡ä¸¢å¤±ä»¥æµ‹è¯•é¢„æµ‹åŠŸèƒ½")
            except:
                print("âš ï¸ æµ‹è¯•æ¨¡å¼å‚æ•°æ ¼å¼é”™è¯¯ï¼Œä½¿ç”¨é»˜è®¤å¸§30-60")
                force_loss_frames = (30, 60)
        
        # åˆ›å»ºè§†é¢‘å†™å…¥å™¨
        fourcc = cv2.VideoWriter_fourcc(*'mp4v')
        out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))
        
        frame_id = 0
        start_time = time.time()
        
        try:
            while True:
                ret, frame = cap.read()
                if not ret:
                    break
                
                self.current_frame_id = frame_id  # ç”¨äºæµ‹è¯•æ¨¡å¼
                
                # YOLOæ£€æµ‹
                detections = self.yolo_detect(frame, force_loss_frames)
                
                # å…³è”æ£€æµ‹ç»“æœ
                detected_tracks = self.associate_detections(detections, frame, frame_id)
                
                # é¢„æµ‹ä¸¢å¤±çš„ç›®æ ‡
                predicted_tracks = self.predict_lost_targets(frame, frame_id)
                
                # ç»˜åˆ¶ç»“æœ
                annotated_frame = self.draw_tracks(frame, detected_tracks, predicted_tracks)
                
                # æ·»åŠ ä¿¡æ¯æ–‡æœ¬
                info_text = f"Frame: {frame_id}, Detected: {len(detected_tracks)}, Predicted: {len(predicted_tracks)}"
                cv2.putText(annotated_frame, info_text, (10, 30), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)
                
                # æµ‹è¯•æ¨¡å¼ä¿¡æ¯
                if test_mode and force_loss_frames and force_loss_frames[0] <= frame_id <= force_loss_frames[1]:
                    test_text = f"TEST MODE: Forced target loss"
                    cv2.putText(annotated_frame, test_text, (10, 60), 
                               cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)
                
                # å†™å…¥è¾“å‡ºè§†é¢‘
                out.write(annotated_frame)
                
                # æ˜¾ç¤ºè¿›åº¦
                if frame_id % 30 == 0 or frame_id == total_frames - 1:
                    progress = (frame_id / total_frames) * 100
                    elapsed = time.time() - start_time
                    current_fps = frame_id / elapsed if elapsed > 0 else 0
                    
                    # åˆ›å»ºè¿›åº¦æ¡
                    bar_length = 30
                    filled_length = int(bar_length * progress / 100)
                    bar = 'â–ˆ' * filled_length + 'â–‘' * (bar_length - filled_length)
                    
                    print(f"   è¿›åº¦: [{bar}] {progress:.1f}% ({frame_id}/{total_frames}), "
                          f"ç”¨æ—¶: {elapsed:.1f}s, å¤„ç†FPS: {current_fps:.1f}, "
                          f"æ£€æµ‹: {len(detected_tracks)}, é¢„æµ‹: {len(predicted_tracks)}")
                
                frame_id += 1
        
        except KeyboardInterrupt:
            print("\nâš ï¸ ç”¨æˆ·ä¸­æ–­å¤„ç†")
        
        finally:
            cap.release()
            out.release()
        
        processing_time = time.time() - start_time
        print(f"âœ… è§†é¢‘å¤„ç†å®Œæˆ!")
        print(f"   è¾“å‡ºæ–‡ä»¶: {output_path}")
        print(f"   å¤„ç†å¸§æ•°: {frame_id}")
        print(f"   å¤„ç†æ—¶é—´: {processing_time:.2f}s")
        print(f"   å¹³å‡FPS: {frame_id / processing_time:.2f}")


def parse_grayscale_template(template_str):
    """è§£æç°åº¦æ¨¡æ¿å­—ç¬¦ä¸²"""
    try:
        # å°è¯•å°†å­—ç¬¦ä¸²è½¬æ¢ä¸ºnumpyæ•°ç»„
        if template_str.startswith('[') and template_str.endswith(']'):
            # å¤„ç†åˆ—è¡¨æ ¼å¼
            import ast
            template_list = ast.literal_eval(template_str)
            template = np.array(template_list, dtype=np.uint8)
        else:
            # å¤„ç†å…¶ä»–æ ¼å¼
            template = np.fromstring(template_str, sep=',', dtype=np.uint8)
            # å‡è®¾æ˜¯25x25çš„æ¨¡æ¿
            if template.size == 625:
                template = template.reshape(25, 25)
        
        return template
    except Exception as e:
        print(f"âš ï¸ è§£æç°åº¦æ¨¡æ¿å¤±è´¥: {e}")
        return None


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='åŸºäºç°åº¦å€¼çš„ç›®æ ‡è¿½è¸ªç³»ç»Ÿ')
    parser.add_argument('--model', '-m', type=str, 
                       default='small_target_detection/yolov8_small_aircraft/weights/best.pt',
                       help='YOLOæ¨¡å‹è·¯å¾„')
    parser.add_argument('--video', '-v', type=str, required=True,
                       help='è¾“å…¥è§†é¢‘è·¯å¾„')
    parser.add_argument('--output', '-o', type=str,
                       help='è¾“å‡ºè§†é¢‘è·¯å¾„ï¼ˆé»˜è®¤è‡ªåŠ¨ç”Ÿæˆï¼‰')
    parser.add_argument('--template', '-t', type=str,
                       help='å±€éƒ¨ç°åº¦å€¼æ¨¡æ¿')
    parser.add_argument('--test', type=str,
                       help='å¯ç”¨æµ‹è¯•æ¨¡å¼ï¼ŒæŒ‡å®šä¸¢å¤±å¸§èŒƒå›´ (æ ¼å¼: start,end ä¾‹å¦‚: 100,150)')
    parser.add_argument('--save-process', action='store_true',
                       help='ä¿å­˜å¤„ç†è¿‡ç¨‹ä¸­çš„ROIå›¾åƒå’Œç°åº¦çŸ©é˜µæ•°æ®åˆ°processç›®å½•')
    
    args = parser.parse_args()
    
    # è®¾ç½®è·¯å¾„
    script_dir = Path(__file__).parent
    model_path = script_dir.parent / args.model
    video_path = Path(args.video)
    
    if not model_path.exists():
        print(f"âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
        return 1
    
    if not video_path.exists():
        print(f"âŒ è§†é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {video_path}")
        return 1
    
    # è®¾ç½®è¾“å‡ºè·¯å¾„
    if args.output:
        output_path = Path(args.output)
    else:
        output_dir = script_dir / "output-vedio"
        output_dir.mkdir(exist_ok=True)
        suffix = "_test" if args.test else ""
        output_path = output_dir / f"tracked{suffix}_{video_path.name}"
    
    try:
        # åˆ›å»ºè¿½è¸ªå™¨
        tracker = GrayscaleTracker(str(model_path), save_process=args.save_process)
        
        # è®¾ç½®ç°åº¦æ¨¡æ¿
        if args.template:
            template = parse_grayscale_template(args.template)
            if template is not None:
                tracker.set_local_grayscale_template(template)
            else:
                print("âš ï¸ ä½¿ç”¨é»˜è®¤æ¢¯åº¦é¢„æµ‹ç­–ç•¥")
        else:
            print("â„¹ï¸ æœªæä¾›ç°åº¦æ¨¡æ¿ï¼Œä½¿ç”¨åŸºäºæ¢¯åº¦çš„é¢„æµ‹æ–¹æ³•")
        
        # å¤„ç†è§†é¢‘
        result_info = tracker.process_video(str(video_path), str(output_path) if args.output else None, test_mode=args.test)
        
        print(f"\nğŸ‰ è¿½è¸ªå®Œæˆï¼")
        
        if args.test:
            print(f"ğŸ§ª æµ‹è¯•æ¨¡å¼å®Œæˆï¼Œæ£€æŸ¥è§†é¢‘ä¸­çš„çº¢è‰²é¢„æµ‹æ¡†")
        
        # è¾“å‡ºä¿¡æ¯ç”±process_videoæ–¹æ³•å†…éƒ¨å¤„ç†ï¼Œä¸éœ€è¦é‡å¤è¾“å‡º
        
    except Exception as e:
        print(f"âŒ å¤„ç†å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return 1
    
    return 0


if __name__ == "__main__":
    exit(main())
